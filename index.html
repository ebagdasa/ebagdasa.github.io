<html lang="en">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-90613073-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-90613073-3');
    </script>

    <title>Eugene Bagdasaryan's website</title>
    <!--Import Google Icon Font-->
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons"
          rel="stylesheet">
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Unna">
    <style>
        h3 {
            font-family: 'Unna', serif;
            font-size: 48px;
        }
    </style>

    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="assets/css/materialize.min.css">
    <!--Let browser know website is optimized for mobile-->
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <link rel="shortcut icon" type="image/png"
          href="assets/images/favicon.ico"/>

</head>
<body>

<div class="container">

    <div class="row">
        <div class="row center-align">
            <div class="push-s1 col s10 m4 l3 ">
                <img src="assets/images/zhenya_new.jpg" alt="image"
                     style="height: 150px;margin-top: 30px;"/>
            </div>

            <div class="col push-s1 s10 m6 l6">
                <div class="row center-align">
                    <h3>Eugene Bagdasaryan</h3>
                </div>
                <div class="divider"></div>
                <div class="row center-align">
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank"
                           href="assets/files/eugene_bagdasaryan_resume.pdf"
                           class="lighten-1 waves-effect waves-teal btn-flat">Resume</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank" href="mailto:eugene@cs.cornell.edu"
                           class="lighten-1 waves-effect waves-teal btn-flat">Email</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank"
                           href="https://scholar.google.com/citations?user=_MfoOC8AAAAJ&hl=en"
                           class="lighten-1 waves-effect waves-teal btn-flat">Scholar</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank" href="https://twitter.com/ebagdasa"
                           class="lighten-1 waves-effect waves-teal btn-flat">Twitter</a>
                    </div>
                </div>

            </div>
        </div>
        <div class="divider"></div>
        <div class="col push-s1 s10 m6 l6">

            <div class="section">
                <p><i class="material-icons">info</i> <b>I am on the job market this year, reach me out: eugene@cs.cornell.edu </b>
                </p>
                <div class="row center-align">
<!--                    <div class="col s6 m1 " >-->
<!--                        <a target="_blank"-->
<!--                           href="assets/files/eugene_bagdasaryan_cover_letter.pdf"-->
<!--                           class="lighten-1 waves-effect waves-teal btn-flat">CL</a>-->
<!--                    </div>-->
                    <div class="col s6 m3 " >
                        <a target="_blank"
                           href="assets/files/eugene_bagdasaryan_resume.pdf"
                           class="lighten-1 waves-effect waves-teal btn-flat">Resume</a>
                    </div>


                    <div class="col s6 m3" >
                        <a target="_blank" href="assets/files/eugene_bagdasaryan_research_statement.pdf"
                           class="lighten-1 waves-effect waves-teal btn-flat">Research</a>
                    </div>

                </div>

                <p><i>Upcoming and past seminar talks: Columbia CS (Apr'23), Boston University CDS (Apr'23),  UW Allen School CSE (Mar'23),
                    McGill CS (Mar'23), CISPA (Feb'23), UMass Manning CICS (Feb'23), UCLA Samueli CS (Jan'23).</i></p>
                <h5>Bio</h5>
                <p style="text-align: justify; text-justify: inter-word;">
                    I am a CS PhD candidate at Cornell Tech and an <a target="_blank" href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2021">Apple AI/ML PhD Scholar</a>

                    advised by
                    <a target="_blank"
                           href="https://www.cs.cornell.edu/~shmat/">Vitaly
                    Shmatikov</a> and
                     <a target="_blank" href="https://destrin.tech.cornell.edu/">Deborah
                        Estrin</a>.
                    I study security and privacy in emerging AI-based systems under real-life conditions and attacks.
                </p>
                <p style="text-align: justify; text-justify: inter-word;">
                    My research goal is to build ethical, safe, and private
                    machine learning systems &ndash; while keeping these systems practical and useful.
                    Recently, we demonstrated security drawbacks of Federated
                    Learning (<a target="_blank"
                                 href="https://arxiv.org/abs/1807.00459">AISTATS'20</a>)
                    and fairness implications of Differentially Private Deep
                    Learning (<a target="_blank"
                                 href="https://arxiv.org/abs/1905.12101">NeurIPS'19</a>).
                    We also proposed a framework for backdoor
                    attacks and defenses (<a target="_blank"
                                             href="https://arxiv.org/abs/2005.03823">USENIX'21</a>) and a new attack (<a target="_blank"
                                             href="https://arxiv.org/abs/2112.05224">S&P'22</a>)
                    that modifies large language models and spins the output for <a target="_blank"
                                                                                         href="https://venturebeat.com/2021/12/14/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused">Propaganda-as-a-Service</a>.
                </p>
                <p style="text-align: justify; text-justify: inter-word;">
                    A big focus of my work is data privacy &ndash; I study methods that enable new applications while protecting users. We proposed
                    <a target="_blank" href="https://ancile-project.github.io/">Ancile</a>
                    &ndash; a framework for language-level control over data
                    usage. At Google, I worked on a new algorithm for building private heatmaps (<a target="_blank"
                                             href="https://arxiv.org/abs/2111.02356">PETS'22</a>).
                    At Apple, I developed a novel way to obtain good tokenizers for Private Federated Learning (<a target="_blank"
                                             href="https://openreview.net/forum?id=rhz7nqYfF-q">FL4NLP@ACL'22</a>).
                    Before starting my PhD, I received an engineer specialist degree from <a target="_blank"
                                                           href="https://en.wikipedia.org/wiki/Bauman_Moscow_State_Technical_University">Baumanka</a>
                    and worked at Cisco on OpenStack networking as a QA Engineer.
                </p>

                <p>I grew up in Tashkent, Uzbekistan. In my free time I play water polo and spend time with family.</p>

                <a rel="me" href="https://sigmoid.social/@eugene"></a>
            </div>
            <div class="section">
                <h5>Research papers</h5>
                <ul class="collapsible popout" data-collapsible="expandable" >
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">map</i>Towards Sparse Federated Analytics: Location Heatmaps under Distributed Differential Privacy with Secure Aggregation <b>PETS'22</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Eugene Bagdasaryan, Peter Kairouz, Stefan Mellem, Adri&#224; Gasc&#243;n, Kallista Bonawitz, Deborah Estrin, and Marco Gruteser</i>
                            <p>A new algorithm for building heatmaps with local-like differential privacy. </p>
                             <b>Work done at Google.</b>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2111.02356"
                                     class="collection-item">[PDF]</a>,
                                <a target="_blank"
                                   href="https://github.com/google-research/federated/tree/master/analytics/location_heatmaps"
                                   class="collection-item">[Code]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">campaign</i>Spinning Language Models: Risks of Propaganda-as-a-Service and Countermeasures <b>S&P'22</b>
                        </div>
                        <div class="collapsible-body" style="justify-content: space-between; align-items: center;">
                            <i>Eugene Bagdasaryan and Vitaly Shmatikov</i>
                            <p>We discover new capabilities of large language models to express attacker-chosen opinions on certain topics while performing tasks like summarization, translation, and language generation.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2112.05224"
                                     class="collection-item">[PDF]</a>,
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/propaganda_as_a_service"
                                   class="collection-item">[Code]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">spellcheck</i>Training a Tokenizer for Free with Private Federated Learning <b>FL4NLP@ACL'22</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan, Congzheng Song, Rogier van Dalen, Matt Seigel, and &#193;ine Cahill</i>
                            <p>Tokenization is an important part of training a good language model, however in private federated learning where user data are not available generic tokenization methods reduce performance. We show how to obtain a good tokenizer without spending additional privacy budget.</p>
                            <b>Work done at Apple. Best paper runner-up award.</b>
                            <span><a target="_blank"
                                     href="https://openreview.net/forum?id=rhz7nqYfF-q"
                                     class="collection-item">[PDF]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">visibility_off</i>
                            Blind Backdoors in Deep Learning Models <b> USENIX'21</b>
                        </div>
                        <div class="collapsible-body">

                            <i>Eugene Bagdasaryan and Vitaly Shmatikov</i>
                            <p>We propose a novel attack that injects complex
                                and semantic
                                backdoors without access to the training data or
                                the model and evades all known
                                defenses.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2005.03823"
                                     class="collection-item">[PDF]</a>,
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/backdoors101"
                                   class="collection-item">[Code]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">smartphone</i>How To
                            Backdoor Federated Learning <span><b>AISTATS'20</b></span>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov</i>
                            <p>We introduce a constrain-and-scale attack,
                                a form of data poisoning, that can stealthily
                                inject
                                a backdoor into one of the participating models
                                during a single round of Federated Learning
                                training.
                                This attack can avoid proposed defenses and
                                propagate the backdoor to a global server
                                that will distribute the compromised model to
                                other participants.</p>
                            <a target="_blank"
                               href="https://arxiv.org/abs/1807.00459"
                               class="collection-item">[PDF]</a>,
                            <a target="_blank"
                               href="https://github.com/ebagdasa/backdoor_federated_learning"
                               class="collection-item">
                                [Code]</a>.
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">local_hospital</i>Salvaging
                            Federated Learning by Local Adaptation <b>Preprint</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov</i>
                            <p>Recovering participants' performance on their
                                data when using federated learning with
                                robustness and privacy techniques.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2002.04758"
                                     class="collection-item">[Paper]</a>,
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/federated_adaptation"
                                   class="collection-item">[Code]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">done</i>Ancile: Enhancing
                            Privacy for Ubiquitous Computing with Use-Based
                            Privacy <b>WPES'19</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan, Griffin Berlstein, Jason Waterman, Eleanor Birrell, Nate Foster, Fred B. Schneider, and Deborah Estrin</i>
                            <p>A novel platform that enables control over
                                application's data usage with language level
                                policies
                                and implementing use-based privacy.</p>
                            <span><a target="_blank"
                                     href="assets/files/ancile.pdf"
                                     class="collection-item">[PDF]</a>,
                                <a target="_blank"
                                   href="https://github.com/ancile-project/ancile"
                                   class="collection-item">[Code]</a>,
                                <a target="_blank"
                                   href="https://docs.google.com/presentation/d/1a2zkEvXLzWJ-CWu2AGl_Bc2pUwKEnnoEKyc17MZbmFQ/edit?usp=sharing"
                                   class="collection-item">[Slides]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">face</i>Differential
                            Privacy Has Disparate Impact on Model Accuracy <b>NeurIPS'19</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan and Vitaly Shmatikov</i>
                            <p>This project discusses a new trade off between
                                privacy and fairness. We observe
                                that training a Machine Learning model with
                                Differential Privacy reduces accuracy on
                                underrepresented groups.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/1905.12101"
                                     class="collection-item">[NeurIPS, 2019]</a>,
                            <a target="_blank"
                               href="https://github.com/ebagdasa/differential-privacy-vs-fairness"
                               class="collection-item">
                                [Code]</a>.
                            </span>
                        </div>
                    </li>


                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">memory</i>X-containers: Breaking down barriers to improve performance and isolation of cloud-native containers <b>ASPLOS'19</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Zhiming Shen, Zhen Sun, Gur-Eyal Sela, Eugene Bagdasaryan, Christina Delimitrou, Robbert Van Renesse, and Hakim Weatherspoon </i>
                            <p>A fast and compact cloud-native implementation of containers.
                            </p>
                            <a target="_blank"
                               href="https://dl.acm.org/doi/abs/10.1145/3297858.3304016"
                               class="collection-item">[PDF]</a>.
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">extension</i>Openrec: A
                            modular framework for extensible and adaptable
                            recommendation algorithms <b>WSDM'18</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Longqi Yang, Eugene Bagdasaryan, Joshua Gruenstein, Cheng-Kang Hsieh, and Deborah Estrin</i>
                            <p>An open and modular Python framework that
                                supports extensible and adaptable research in
                                recommender systems.
                            </p>
                            <a target="_blank"
                               href="https://dl.acm.org/citation.cfm?id=3159681"
                               class="collection-item">[PDF]</a>,
                            <a target="_blank"
                               href="https://github.com/ylongqi/openrec"
                               class="collection-item">[Code]</a>.
                        </div>
                    </li>
                </ul>
            </div>
        </div>

        <div class="col push-s1 s10 m5 l5 push-l1 push-m1">
            <div class="section">
                <h5>Recent news</h5>
                <ul class="collection">
                    <li class="collection-item">
                        <b>Oct 2022</b>, <a target="_blank" href="https://pluralistic.net/2022/10/21/let-me-summarize/">Cory Doctorow</a>  and
                        <a target="_blank" href="https://www.schneier.com/blog/archives/2022/10/adversarial-ml-attack-that-secretly-gives-a-language-model-a-point-of-view.html?utm_source=dlvr.it&utm_medium=twitter">Bruce Schneier</a> wrote about our research on model spinning.
                    </li>
                    <li class="collection-item">
                        <b>May 2022</b>, a <a target="_blank" href="https://arxiv.org/abs/2111.02356">paper</a> on location heatmaps was accepted to PETS'22.
                    </li>
                    <li class="collection-item">
                        <b>Apr 2022</b>, a Propaganda-as-a-Service <a target="_blank"
                                     href="https://arxiv.org/abs/2112.05224">paper</a> accepted to <a target="_blank" href="https://www.computer.org/csdl/proceedings-article/sp/2022/131600b532/1CIO7BDk9sA">S&P'22</a>.
                    </li>
                    <li class="collection-item">
                        <b>Mar 2022</b>, a <a target="_blank" href="https://openreview.net/forum?id=rhz7nqYfF-q">paper</a> on tokenizers in FL accepted to FL4NLP workshop at ACL'22, got best paper runner-up award.
                    </li>
                    <li class="collection-item">
                        <b>Dec 2021</b>, our <a target="_blank"
                                     href="https://arxiv.org/abs/2112.05224">research</a> on Propaganda-as-a-Service was covered by <a target="_blank" href="https://venturebeat.com/2021/12/14/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused/">VentureBeat</a>.
                    </li>
                    <li class="collection-item">
                        <b>Aug 2021</b>, our blind backdoors <a target="_blank" href="https://arxiv.org/abs/2005.03823">paper</a> was covered on <a target="_blank" href="https://www.zdnet.com/article/cornell-university-researchers-discover-code-poisoning-attack/">ZDNet</a>.
                    </li>
                    <li class="collection-item">
                        <b>Summer 2021</b>, interned at Apple with <a
                            target="_blank"
                            href="https://www.vandalen.uk">Rogier van Dalen</a>
                        working on Private Federated Learning for Large Language Models.
                    </li>
                    <li class="collection-item">
                        <b>Apr 2021</b>, received Apple <a target="_blank" href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2021">fellowship</a>. Mentors:
                        <a target="_blank" href="https://www.vandalen.uk">Rogier van Dalen</a> and
                        <a target="_blank" href="http://kunaltalwar.org/">Kunal Talwar</a>.
                    </li>
                    <li class="collection-item">
                        <b>Feb 2021</b>, our <a target="_blank"
                                            href="https://arxiv.org/abs/2005.03823">paper</a>
                        on blind backdoors accepted to USENIX Security'21.
                    </li>
                    <li class="collection-item">
                        <b>Jan 2021</b>, presented our work at Microsoft
                        Research.
                    </li>
                    <li class="collection-item">
                        <b>Nov 2020</b>, open sourced our new
                        <a target="_blank"
                           href="https://github.com/ebagdasa/backdoors101">framework</a>
                        for research on backdoors in deep learning.
                    </li>
                    <li class="collection-item">
                        <b>Jul 2020</b>, presented our <a target="_blank"
                                                          href="https://arxiv.org/abs/2002.04758">work</a>
                        on local adaption for Federated Learning at Google.
                    </li>
                    <li class="collection-item">
                        <b>Jun 2020</b>, Ancile project was discussed in <a
                            target="_blank"
                            href="https://news.cornell.edu/stories/2020/06/platform-empowers-users-control-their-personal-data">Cornell
                        Chronicle</a>.
                    </li>
                    <li class="collection-item">
                        <b>Summer 2020</b>, interned at Google Research with <a
                            target="_blank"
                            href="http://www.winlab.rutgers.edu/~gruteser/">Marco
                        Gruteser</a>, <a
                            target="_blank"
                            href="https://kairouzp.github.io">Peter Kairouz</a>,
                        and <a target="_blank"
                               href="https://research.google/people/105175/">Kaylee
                        Bonawitz</a>, focused on Federated Learning and
                        Analytics.
                    </li>
                    <li class="collection-item">
                        <b>Jan 2020</b>, our <a target="_blank"
                                                href="https://arxiv.org/abs/1807.00459">attack</a>
                        on federated learning was accepted to AISTATS'20!
                    </li>
                    <li class="collection-item">
                        <b>Nov 2019</b>, passed A exam (pre-candidacy):
                        "Evaluating privacy preserving techniques in machine
                        learning."
                    </li>
                    <li class="collection-item">
                        <b>Sep 2019</b>, our <a target="_blank"
                                                href="https://arxiv.org/abs/1905.12101">paper</a>
                        about
                        differential privacy impact on model fairness was
                        accepted to NeurIPS'19.
                    </li>
                    <li class="collection-item">
                        <b>Aug 2019</b>, our work on the use-based privacy
                        system <a target="_blank"
                                  href="assets/files/ancile.pdf">Ancile</a> was
                        accepted to CCS WPES'19.
                    </li>
                    <li class="collection-item">
                        <b>Aug 2019</b>, presented at <a target="_blank"
                                                         href="http://privaci.info/ci_symposium.html">Contextual
                        Integrity Symposium</a>
                        on contextual recommendation sharing.
                    </li>
                    <li class="collection-item">
                        <b>June 2019</b>, <a target="_blank"
                                             href="https://www.dli.tech.cornell.edu/doctoralfellows">Digital
                        Life Initiative</a> fellow 2019-2020.
                    </li>
                    <li class="collection-item">
                        <b>Summer 2018</b>, interned at Amazon Research with
                        Pawel Matykiewicz and Amber Roy Chowdhury.
                    </li>
                    <li class="collection-item">
                        <b>Sep 2017</b>,
                        <a target="_blank"
                           href="https://medium.com/@nycmedialab/data-for-good-bloomberg-supports-data-scientists-work-with-nonprofits-and-municipalities-to-solve-6d9ce6360ea8">
                            Bloomberg Data for Good</a> fellow 2017.
                    </li>
                </ul>

            </div>
        </div>
        <!--        <div class="col push-s1 s10 m6 l6">-->
        <!--            <div class="divider"></div>-->

        <!--        </div>-->
    </div>
        <div class="footer-copyright right-align"><i>last updated Nov 2022.</i></div>
</div>


<!-- Compiled and minified JavaScript -->
<script type="text/javascript" src="assets/js/jquery-3.4.1.min.js"></script>
<script src="assets/js/materialize.min.js"></script>
<script type="text/javascript" src="assets/js/script.js"></script>
</body>

</html>
