<html lang="en">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-90613073-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-90613073-3');
    </script>

    <title>Eugene Bagdasaryan's website</title>
    <!--Import Google Icon Font-->
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons"
          rel="stylesheet">
<!--    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />-->
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Unna">

    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="assets/css/materialize.min.css">
    <!--Let browser know website is optimized for mobile-->
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <link rel="shortcut icon" type="image/png"
          href="assets/images/favicon.ico"/>
    <style>
        h3 {
            font-family: 'Unna', serif;
            font-size: 48px;
        }
        a { color: #1a7caa; }
    </style>

</head>
<body>

<div class="container">

    <div class="row">
        <div class="row center-align">
            <div class="push-s1 col s10 m4 l3 ">
                <img src="assets/images/zhenya.jpg" alt="image"
                     style="height: 190px;margin-top: 30px;"/>
            </div>

            <div class="col push-s1 s10 m6 l6">
                <div class="row center-align">
                    <h3>Eugene Bagdasaryan</h3>

                </div>
                <div class="divider"></div>
                <div class="row center-align">
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank"
                           href="assets/files/eugene_bagdasaryan_resume.pdf"
                           class="lighten-1 waves-effect waves-teal btn-flat">Resume</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank" href="mailto:eugene@cs.umass.edu"
                           class="lighten-1 waves-effect waves-teal btn-flat">Email</a>
                    </div>

                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank"
                           href="https://scholar.google.com/citations?user=_MfoOC8AAAAJ&hl=en"
                           class="lighten-1 waves-effect waves-teal btn-flat">Scholar</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank" href="https://twitter.com/ebagdasa"
                           class="lighten-1 waves-effect waves-teal btn-flat">Twitter</a>
                    </div>
                </div>

            </div>
        </div>
        <div class="divider"></div>
        <div class="col push-s1 s10 m6 l6">

            <div class="section">


<!--                <h5>Bio</h5>-->
                <p style="text-align: justify; text-justify: inter-word;">
                    Eugene is an Assistant Professor at <a target="_blank" href="https://www.cics.umass.edu/">UMass Amherst CICS</a>.
                    Eugene's work focuses on security and
                    privacy in emerging AI-based systems under real-life conditions and attacks.

                    He completed his PhD at <a target="_blank"
                                                           href="https://tech.cornell.edu/">Cornell Tech</a> advised by
                    <a target="_blank"
                           href="https://www.cs.cornell.edu/~shmat/">Vitaly
                    Shmatikov</a> and
                     <a target="_blank" href="https://destrin.tech.cornell.edu/">Deborah
                        Estrin</a>. Eugene's research was recognized by <a target="_blank" href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2021">Apple Scholars in AI/ML</a>
                    and
                    <a target="_blank"
                       href="https://www.dli.tech.cornell.edu/doctoralfellows">Digital
                        Life Initiative</a> fellowships.
                    He received an engineering degree from
                    <a target="_blank" href="https://en.wikipedia.org/wiki/Bauman_Moscow_State_Technical_University">Baumanka</a>
                    and worked at Cisco as a software engineer. Eugene has extensive industry experience
                    (Cisco, Amazon, Apple) and spends part of his time as a Research Scientist at Google.
                </p>
                <p style="text-align: justify; text-justify: inter-word;">

<!--                    My research goal is to build ethical, safe, and private-->
<!--                    machine learning systems &ndash; while keeping these systems practical and useful.-->
                    <b>Security: </b> He worked on backdoor <a target="_blank"
                                 href="https://arxiv.org/abs/1807.00459">attacks in federated learning</a>
                   and proposed new frameworks <a target="_blank" href="https://arxiv.org/abs/2005.03823">Backdoors101</a> and
                    <a target="_blank" href="https://arxiv.org/abs/2302.04977">Mithridates</a>,
                    and a new <a target="_blank" href="https://arxiv.org/abs/2112.05224">attack</a> on generative language models
                    covered by <a target="_blank" href="https://venturebeat.com/2021/12/14/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused">VentureBeat</a>
                    and <a target="_blank" href="https://www.economist.com/science-and-technology/2023/04/05/it-doesnt-take-much-to-make-machine-learning-algorithms-go-awr">The Economist</a>.
                    Recent work includes studies on vulnerabilities in multi-modal systems: <a target="_blank"
                    href="https://arxiv.org/abs/2407.08970"
                    class="collection-item">instruction injections</a>, <a target="_blank" href="https://arxiv.org/abs/2308.11804"
                                                                                            class="collection-item">adversarial illusions</a>
                    and adding <a target="_blank" href="https://arxiv.org/abs/2406.15213"
                                                                         class="collection-item">biases to text-to-image models</a>.


                </p>
                <p style="text-align: justify; text-justify: inter-word;">
                    <b>Privacy: </b> Eugene worked on the <a target="_blank" href="https://ancile-project.github.io/">Air Gap</a>  privacy protection for LLM Agents
                    and <a target="_blank"
                           href="https://arxiv.org/abs/2408.02373">operationalizing</a> Contextual Integrity.
                    He worked on aspects of differential privacy including fairness <a target="_blank"
                                    href="https://arxiv.org/abs/1905.12101">trade-offs</a>,
                    applications to <a target="_blank"  href="https://arxiv.org/abs/2111.02356">location heatmaps</a>,
                    and tokenization <a target="_blank" href="https://openreview.net/forum?id=rhz7nqYfF-q">methods</a> for private federated learning.
                    Additionally he helped to build the <a target="_blank"
                                             href="https://github.com/ancile-project/ancile">Ancile</a> system that enforces use-based privacy of user data.

                </p>

                <p>Eugene grew up in <a target="_blank" href="https://g.co/kgs/8TvnWj">Tashkent</a> and plays water polo.</p>

                <p>
                <b>Announcement 1:</b>
                I am looking for PhD students (<a target="_blank" href="https://www.cics.umass.edu/degree-program/doctoral">apply</a>)  and post-docs to work on
                attacks on LLM agents and generative models. Please reach out over email!
                </p>
                <p><b>Announcement 2:</b> We will be holding a <a target="_blank" href="classes/seminar.html">seminar CS 692PA</a> on Privacy and Security for GenAI models, please sign up if you are interested.

                </p>
                <a rel="me" href="https://sigmoid.social/@eugene"></a>

            </div>
            <div class="section">
                <h5>Papers</h5>

                <ul class="collapsible popout" data-collapsible="expandable" >
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">format_paint</i>Operationalizing Contextual Integrity in Privacy-Conscious Assistants, <b>Preprint'24</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Sahra Ghalebikesabi, Eugene Bagdasaryan, Ren Yi, Itay Yona, Ilia Shumailov, Aneesh Pappu,
                                Chongyang Shi, Laura Weidinger, Robert Stanforth, Leonard Berrada, Pushmeet Kohli, Po-Sen Huang, Borja Balle</i>
                            <p>Contextual Integrity can be applied to help personal assistants protect user privacy, we study how to instruct LLMs to
                            follow CI.</p>
                            <b>Work done at Google<i class="material-icons cyan-text text-darken-4">android</i>.</b>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2408.02373"
                                     class="collection-item">[PDF]</a>

                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">image_search</i>Injecting Bias in Text-To-Image Models via Composite-Trigger Backdoors<b>Preprint'24</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Ali Naseh, Jaechul Roh, Eugene Bagdasaryan, Amir Houmansadr</i>
                            <p>Can text-to-image models be impacted by the adversary injected biases?</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2406.15213"
                                     class="collection-item">[PDF]</a>
                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">blur_on</i>UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI
                            <b>Preprint'24</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Ilia Shumailov, Jamie Hayes, Eleni Triantafillou, Guillermo Ortiz-Jimenez, Nicolas Papernot, Matthew Jagielski, Itay Yona, Heidi Howard, Eugene Bagdasaryan
                            </i>
                            <p>Unlearning large concepts can either be easily recovered or will impact model performance.</p>
                            <b>Work done at Google<i class="material-icons cyan-text text-darken-4">android</i>.</b>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2407.00106"
                                     class="collection-item">[PDF]</a>

                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">integration_instructions</i>Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions<b>Preprint'24</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Tingwei Zhang, Collin Zhang, John X. Morris, Eugene Bagdasaryan, Vitaly Shmatikov</i>
                            <p>How an adversary can inject hidden instructions into a visual language model?</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2407.08970"
                                     class="collection-item">[PDF]</a>
                            </span>
                        </div>
                    </li>



                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">flutter_dash</i>Air Gap: Protecting Privacy-Conscious Conversational Agents<b>CCS'24</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Eugene Bagdasaryan, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong Oh, Borja Balle, Daniel Ramage</i>
                            <p>Can you make an agent that knows when to share the user data depending on context? Can it be protected from adversaries trying to extract that data. </p>
                            <b>Work done at Google<i class="material-icons cyan-text text-darken-4">android</i>.</b>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2405.05175"
                                     class="collection-item">[PDF]</a>

                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">scienceoff</i>Mithridates: Boosting Natural Resistance to Backdoor Learning <b>CCS'24</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Eugene Bagdasaryan, Vitaly Shmatikov</i>
                            <p>A novel audit method for poisoning attacks. </p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2302.04977"
                                     class="collection-item">[PDF]</a>
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/mithridates"
                                   class="collection-item">[Code]</a>
                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">auto_fix_high</i> <i class="material-icons yellow-text text-darken-4">emoji_events</i> Adversarial Illusions in Multi-Modal Embeddings <b>Usenix Security'24
                        </b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Tingwei Zhang, Rishi Jha, Eugene Bagdasaryan, Vitaly Shmatikov</i>
                            <p>We propose adversarial alignment in cross-modal settings. <b>Distinguished paper award.</b></p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2308.11804"
                                     class="collection-item">[PDF]</a>,
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/adversarial_illusions"
                                   class="collection-item">[Code]</a>.
                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">forum</i>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs <b>Preprint'23</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</i>
                            <p>A new prompt injection attack that makes LLMs talk like pirates when they see an image or an audio. </p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2307.10490"
                                     class="collection-item">[PDF]</a>
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/multimodal_injection"
                                   class="collection-item">[Code]</a>
                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">map</i>Towards Sparse Federated Analytics: Location Heatmaps under Distributed Differential Privacy with Secure Aggregation <b>PETS'22</b>
                        </div>
                        <div class="collapsible-body" >
                            <i>Eugene Bagdasaryan, Peter Kairouz, Stefan Mellem, Adri&#224; Gasc&#243;n, Kallista Bonawitz, Deborah Estrin, and Marco Gruteser</i>
                            <p>A new algorithm for building heatmaps with local-like differential privacy. </p>
                             <b>Work done at Google.</b>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2111.02356"
                                     class="collection-item">[PDF]</a>
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/federated/tree/master/analytics/location_heatmaps"
                                   class="collection-item">[Updated Code]</a>
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">campaign</i>Spinning Language Models: Risks of Propaganda-as-a-Service and Countermeasures <b>S&P'22</b>
                        </div>
                        <div class="collapsible-body" style="justify-content: space-between; align-items: center;">
                            <i>Eugene Bagdasaryan and Vitaly Shmatikov</i>
                            <p>We discover new capabilities of large language models to express attacker-chosen opinions on certain topics while performing tasks like summarization, translation, and language generation.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2112.05224"
                                     class="collection-item">[PDF]</a>
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/propaganda_as_a_service"
                                   class="collection-item">[Code]</a>
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">spellcheck</i>Training a Tokenizer for Free with Private Federated Learning <b>FL4NLP@ACL'22</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan, Congzheng Song, Rogier van Dalen, Matt Seigel, and &#193;ine Cahill</i>
                            <p>Tokenization is an important part of training a good language model, however in private federated learning where user data are not available generic tokenization methods reduce performance. We show how to obtain a good tokenizer without spending additional privacy budget.</p>
                            <b>Work done at Apple. Best paper runner-up award.</b>
                            <span><a target="_blank"
                                     href="https://openreview.net/forum?id=rhz7nqYfF-q"
                                     class="collection-item">[PDF]</a>
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">visibility_off</i>
                            Blind Backdoors in Deep Learning Models <b> Usenix Security'21</b>
                        </div>
                        <div class="collapsible-body">

                            <i>Eugene Bagdasaryan and Vitaly Shmatikov</i>
                            <p>We propose a novel attack that injects complex
                                and semantic
                                backdoors without access to the training data or
                                the model and evades all known
                                defenses.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2005.03823"
                                     class="collection-item">[PDF]</a>
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/backdoors101"
                                   class="collection-item">[Code]</a>
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">smartphone</i>How To
                            Backdoor Federated Learning <span><b>AISTATS'20</b></span>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov</i>
                            <p>We introduce a constrain-and-scale attack,
                                a form of data poisoning, that can stealthily
                                inject
                                a backdoor into one of the participating models
                                during a single round of Federated Learning
                                training.
                                This attack can avoid proposed defenses and
                                propagate the backdoor to a global server
                                that will distribute the compromised model to
                                other participants.</p>
                            <a target="_blank"
                               href="https://arxiv.org/abs/1807.00459"
                               class="collection-item">[PDF]</a>
                            <a target="_blank"
                               href="https://github.com/ebagdasa/backdoor_federated_learning"
                               class="collection-item">
                                [Code]</a>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center">
                            <i class="material-icons">local_hospital</i>Salvaging
                            Federated Learning by Local Adaptation <b>Preprint</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov</i>
                            <p>Recovering participants' performance on their
                                data when using federated learning with
                                robustness and privacy techniques.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2002.04758"
                                     class="collection-item">[Paper]</a>
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/federated_adaptation"
                                   class="collection-item">[Code]</a>
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">done</i>Ancile: Enhancing
                            Privacy for Ubiquitous Computing with Use-Based
                            Privacy <b>WPES'19</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan, Griffin Berlstein, Jason Waterman, Eleanor Birrell, Nate Foster, Fred B. Schneider, and Deborah Estrin</i>
                            <p>A novel platform that enables control over
                                application's data usage with language level
                                policies
                                and implementing use-based privacy.</p>
                            <span><a target="_blank"
                                     href="assets/files/ancile.pdf"
                                     class="collection-item">[PDF]</a>
                                <a target="_blank"
                                   href="https://github.com/ancile-project/ancile"
                                   class="collection-item">[Code]</a>
                                <a target="_blank"
                                   href="https://docs.google.com/presentation/d/1a2zkEvXLzWJ-CWu2AGl_Bc2pUwKEnnoEKyc17MZbmFQ/edit?usp=sharing"
                                   class="collection-item">[Slides]</a>
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">face</i>Differential
                            Privacy Has Disparate Impact on Model Accuracy <b>NeurIPS'19</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Eugene Bagdasaryan and Vitaly Shmatikov</i>
                            <p>This project discusses a new trade off between
                                privacy and fairness. We observe
                                that training a Machine Learning model with
                                Differential Privacy reduces accuracy on
                                underrepresented groups.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/1905.12101"
                                     class="collection-item">[NeurIPS, 2019]</a>,
                            <a target="_blank"
                               href="https://github.com/ebagdasa/differential-privacy-vs-fairness"
                               class="collection-item">
                                [Code]</a>.
                            </span>
                        </div>
                    </li>


                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">memory</i>X-containers: Breaking down barriers to improve performance and isolation of cloud-native containers <b>ASPLOS'19</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Zhiming Shen, Zhen Sun, Gur-Eyal Sela, Eugene Bagdasaryan, Christina Delimitrou, Robbert Van Renesse, and Hakim Weatherspoon </i>
                            <p>A fast and compact cloud-native implementation of containers.
                            </p>
                            <a target="_blank"
                               href="https://dl.acm.org/doi/abs/10.1145/3297858.3304016"
                               class="collection-item">[PDF]</a>.
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header" style="justify-content: space-between; align-items: center; text-align: center"><i
                                class="material-icons">extension</i>Openrec: A
                            modular framework for extensible and adaptable
                            recommendation algorithms <b>WSDM'18</b>
                        </div>
                        <div class="collapsible-body">
                            <i>Longqi Yang, Eugene Bagdasaryan, Joshua Gruenstein, Cheng-Kang Hsieh, and Deborah Estrin</i>
                            <p>An open and modular Python framework that
                                supports extensible and adaptable research in
                                recommender systems.
                            </p>
                            <a target="_blank"
                               href="https://dl.acm.org/citation.cfm?id=3159681"
                               class="collection-item">[PDF]</a>,
                            <a target="_blank"
                               href="https://github.com/ylongqi/openrec"
                               class="collection-item">[Code]</a>.
                        </div>
                    </li>
                </ul>
            </div>
        </div>

        <div class="col push-s1 s10 m5 l5 push-l1 push-m1">
            <div class="section">
                <h5>Recent news</h5>
                <ul class="collection">
                    <li class="collection-item">
                        <b>July 2024</b>, at CCS'24 we will show how to defend against poisoning without modifying an ML pipeline with <a target="_blank" href="https://arxiv.org/abs/2302.04977">Mithridates</a>.
                    </li>
                    <li class="collection-item">
                        <b>July 2024</b>, Privacy-conscious <a target="_blank"
                                                               href="https://arxiv.org/abs/2405.05175"
                                                              >agents</a> will cautiously appear at CCS'24.
                    </li>
                    <li class="collection-item">
                        <b>May 2024</b>, Adversarial Illusions show comes to Usenix Security'24.
                    </li>
                    <li class="collection-item">
                        <b>Aug 2023</b>, started as a Research Scientist at Google, joining <a target="_blank" href="https://www.cics.umass.edu/">UMass Amherst</a> as Assistant Professor in Fall'24.
                    </li>
                    <li class="collection-item">
                        <b>Jul 2023</b>, Defended PhD thesis:
                        "Untrustworthy Machine Learning."
                    </li>
                    <li class="collection-item">
                        <b>Apr 2023</b>, interviewed by <a target="_blank" href="https://www.economist.com/science-and-technology/2023/04/05/it-doesnt-take-much-to-make-machine-learning-algorithms-go-awry">The Economist</a> on our work studying language models.
                    </li>
                    <li class="collection-item">
                        <b>Oct 2022</b>, <a target="_blank" href="https://pluralistic.net/2022/10/21/let-me-summarize/">Cory Doctorow</a>  and
                        <a target="_blank" href="https://www.schneier.com/blog/archives/2022/10/adversarial-ml-attack-that-secretly-gives-a-language-model-a-point-of-view.html?utm_source=dlvr.it&utm_medium=twitter">Bruce Schneier</a> wrote about our research on model spinning.
                    </li>
                    <li class="collection-item">
                        <b>May 2022</b>, a <a target="_blank" href="https://arxiv.org/abs/2111.02356">paper</a> on location heatmaps was accepted to PETS'22.
                    </li>
                    <li class="collection-item">
                        <b>Apr 2022</b>, a Propaganda-as-a-Service <a target="_blank"
                                     href="https://arxiv.org/abs/2112.05224">paper</a> accepted to <a target="_blank" href="https://www.computer.org/csdl/proceedings-article/sp/2022/131600b532/1CIO7BDk9sA">S&P'22</a>.
                    </li>
<!--                    <li class="collection-item">-->
<!--                        <b>Mar 2022</b>, a <a target="_blank" href="https://openreview.net/forum?id=rhz7nqYfF-q">paper</a> on tokenizers in FL accepted to FL4NLP workshop at ACL'22, got best paper runner-up award.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Dec 2021</b>, our <a target="_blank"-->
<!--                                     href="https://arxiv.org/abs/2112.05224">research</a> on Propaganda-as-a-Service was covered by <a target="_blank" href="https://venturebeat.com/2021/12/14/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused/">VentureBeat</a>.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Aug 2021</b>, our blind backdoors <a target="_blank" href="https://arxiv.org/abs/2005.03823">paper</a> was covered on <a target="_blank" href="https://www.zdnet.com/article/cornell-university-researchers-discover-code-poisoning-attack/">ZDNet</a>.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Summer 2021</b>, interned at Apple with <a-->
<!--                            target="_blank"-->
<!--                            href="https://www.vandalen.uk">Rogier van Dalen</a>-->
<!--                        working on Private Federated Learning for Large Language Models.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Apr 2021</b>, received Apple <a target="_blank" href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2021">fellowship</a>. Mentors:-->
<!--                        <a target="_blank" href="https://www.vandalen.uk">Rogier van Dalen</a> and-->
<!--                        <a target="_blank" href="http://kunaltalwar.org/">Kunal Talwar</a>.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Feb 2021</b>, our <a target="_blank"-->
<!--                                            href="https://arxiv.org/abs/2005.03823">paper</a>-->
<!--                        on blind backdoors accepted to USENIX Security'21.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Jan 2021</b>, presented our work at Microsoft-->
<!--                        Research.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Nov 2020</b>, open sourced our new-->
<!--                        <a target="_blank"-->
<!--                           href="https://github.com/ebagdasa/backdoors101">framework</a>-->
<!--                        for research on backdoors in deep learning.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Jul 2020</b>, presented our <a target="_blank"-->
<!--                                                          href="https://arxiv.org/abs/2002.04758">work</a>-->
<!--                        on local adaption for Federated Learning at Google.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Jun 2020</b>, Ancile project was discussed in <a-->
<!--                            target="_blank"-->
<!--                            href="https://news.cornell.edu/stories/2020/06/platform-empowers-users-control-their-personal-data">Cornell-->
<!--                        Chronicle</a>.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Summer 2020</b>, interned at Google Research with <a-->
<!--                            target="_blank"-->
<!--                            href="http://www.winlab.rutgers.edu/~gruteser/">Marco-->
<!--                        Gruteser</a>, <a-->
<!--                            target="_blank"-->
<!--                            href="https://kairouzp.github.io">Peter Kairouz</a>,-->
<!--                        and <a target="_blank"-->
<!--                               href="https://research.google/people/105175/">Kaylee-->
<!--                        Bonawitz</a>, focused on Federated Learning and-->
<!--                        Analytics.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Jan 2020</b>, our <a target="_blank"-->
<!--                                                href="https://arxiv.org/abs/1807.00459">attack</a>-->
<!--                        on federated learning was accepted to AISTATS'20!-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Nov 2019</b>, passed A exam (pre-candidacy):-->
<!--                        "Evaluating privacy preserving techniques in machine-->
<!--                        learning."-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Sep 2019</b>, our <a target="_blank"-->
<!--                                                href="https://arxiv.org/abs/1905.12101">paper</a>-->
<!--                        about-->
<!--                        differential privacy impact on model fairness was-->
<!--                        accepted to NeurIPS'19.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Aug 2019</b>, our work on the use-based privacy-->
<!--                        system <a target="_blank"-->
<!--                                  href="assets/files/ancile.pdf">Ancile</a> was-->
<!--                        accepted to CCS WPES'19.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Aug 2019</b>, presented at <a target="_blank"-->
<!--                                                         href="http://privaci.info/ci_symposium.html">Contextual-->
<!--                        Integrity Symposium</a>-->
<!--                        on contextual recommendation sharing.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>June 2019</b>, <a target="_blank"-->
<!--                                             href="https://www.dli.tech.cornell.edu/doctoralfellows">Digital-->
<!--                        Life Initiative</a> fellow 2019-2020.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Summer 2018</b>, interned at Amazon Research with-->
<!--                        Pawel Matykiewicz and Amber Roy Chowdhury.-->
<!--                    </li>-->
<!--                    <li class="collection-item">-->
<!--                        <b>Sep 2017</b>,-->
<!--                        <a target="_blank"-->
<!--                           href="https://medium.com/@nycmedialab/data-for-good-bloomberg-supports-data-scientists-work-with-nonprofits-and-municipalities-to-solve-6d9ce6360ea8">-->
<!--                            Bloomberg Data for Good</a> fellow 2017.-->
<!--                    </li>-->
                </ul>

            </div>
        </div>
        <!--        <div class="col push-s1 s10 m6 l6">-->
        <!--            <div class="divider"></div>-->

        <!--        </div>-->
    </div>
        <div class="footer-copyright right-align"><i>last updated Sep 2024.</i></div>
</div>


<!-- Compiled and minified JavaScript -->
<script type="text/javascript" src="assets/js/jquery-3.4.1.min.js"></script>
<script src="assets/js/materialize.min.js"></script>
<script type="text/javascript" src="assets/js/script.js"></script>
</body>

</html>
