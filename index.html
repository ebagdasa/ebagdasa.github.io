<html lang="en">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-90613073-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-90613073-3');
    </script>

    <title>Eugene Bagdasaryan's website</title>
    <!--Import Google Icon Font-->
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons"
          rel="stylesheet">
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Unna">
    <style>
        h3 {
            font-family: 'Unna', serif;
            font-size: 48px;
        }
    </style>

    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="assets/css/materialize.min.css">
    <!--Let browser know website is optimized for mobile-->
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <link rel="shortcut icon" type="image/png"
          href="assets/images/favicon.ico"/>

</head>
<body>

<div class="container">

    <div class="row">
        <div class="row center-align">
            <div class="push-s1 col s10 m4 l3 ">
                <img src="assets/images/zhenya_new.jpg" alt="image"
                     style="height: 150px;margin-top: 30px;"/>
            </div>

            <div class="col push-s1 s10 m6 l6">
                <div class="row center-align">
                    <h3>Eugene Bagdasaryan</h3>
                </div>
                <div class="divider"></div>
                <div class="row center-align">
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank"
                           href="assets/files/EugeneBagdasaryanResume.pdf"
                           class="lighten-1 waves-effect waves-teal btn-flat">Resume</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank" href="mailto:eugene@cs.cornell.edu"
                           class="lighten-1 waves-effect waves-teal btn-flat">Email</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank"
                           href="https://scholar.google.com/citations?user=_MfoOC8AAAAJ&hl=en"
                           class="lighten-1 waves-effect waves-teal btn-flat">Scholar</a>
                    </div>
                    <div class="col s6 m3" style="padding-bottom: 20px;">
                        <a target="_blank" href="https://twitter.com/ebagdasa"
                           class="lighten-1 waves-effect waves-teal btn-flat">Twitter</a>
                    </div>
                </div>

            </div>
        </div>
        <div class="divider"></div>
        <div class="col push-s1 s10 m6 l6">

            <div class="section">
                <h5>Bio</h5>
                <p style="text-align: justify; text-justify: inter-word;">
                    I am a CS PhD candidate at Cornell Tech, working on privacy
                    and security
                    in Machine Learning and advised by
                    <a target="_blank" href="https://destrin.tech.cornell.edu/">Deborah
                        Estrin</a>
                    and <a target="_blank"
                           href="https://www.cs.cornell.edu/~shmat/">Vitaly
                    Shmatikov</a>.
                </p>
                <p style="text-align: justify; text-justify: inter-word;">
                    My research goal is to build ethical, safe, and private
                    Machine Learning systems.
                    In our work, we demonstrate security drawbacks of Federated
                    Learning (<a target="_blank"
                                 href="https://arxiv.org/abs/1807.00459">AISTATS'20</a>)
                    and fairness implications of Differentially Private Deep
                    Learning (<a target="_blank"
                                 href="https://arxiv.org/abs/1905.12101">NeurIPS'19</a>).
                    Recently, we have proposed a novel framework for backdoor
                    attacks and defenses (<a target="_blank"
                                             href="https://arxiv.org/abs/2005.03823">USENIX'21</a>).
                </p>
                <p style="text-align: justify; text-justify: inter-word;">
                    Earlier, I worked on
                    <a target="_blank" href="https://ancile-project.github.io/">Ancile</a>
                    &ndash; a framework for language-level control over data
                    usage,
                    and <a target="_blank" href="https://openrec.ai">OpenRec</a>
                    &ndash;
                    a modular library for deep recommender systems. Amazon and Google Research hosted me for summer internships.
                    Before starting my PhD, I received engineering degree from <a target="_blank"
                                                           href="https://en.wikipedia.org/wiki/Bauman_Moscow_State_Technical_University">Baumanka</a>
                    and worked at Cisco on OpenStack networking.

                </p>

                <p>In my free time I play water polo and (used to...)
                    travel.</p>
            </div>
            <div class="section">
                <h5>Research papers</h5>
                <ul class="collapsible" data-collapsible="accordion">
                    <li>
                        <div class="collapsible-header">
                            <i class="material-icons">visibility_off</i>Blind
                            Backdoors in Deep Learning Models
                        </div>
                        <div class="collapsible-body">
                            <p>We propose a novel attack that injects complex
                                and semantic
                                backdoors without access to the training data or
                                the model and evades all known
                                defenses.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2005.03823"
                                     class="collection-item">[USENIX, 2021]</a>,
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/backdoors101"
                                   class="collection-item">[Code]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header">
                            <i class="material-icons">smartphone</i>How To
                            Backdoor Federated Learning
                        </div>
                        <div class="collapsible-body">
                            <p>We introduce a constrain-and-scale attack,
                                a form of data poisoning, that can stealthily
                                inject
                                a backdoor into one of the participating models
                                during a single round of Federated Learning
                                training.
                                This attack can avoid proposed defenses and
                                propagate the backdoor to a global server
                                that will distribute the compromised model to
                                other participants.</p>
                            <a target="_blank"
                               href="https://arxiv.org/abs/1807.00459"
                               class="collection-item">[AISTATS, 2020]</a>,
                            <a target="_blank"
                               href="https://github.com/ebagdasa/backdoor_federated_learning"
                               class="collection-item">
                                [Code]</a>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header">
                            <i class="material-icons">local_hospital</i>Salvaging
                            Federated Learning by Local Adaptation
                        </div>
                        <div class="collapsible-body">
                            <p>Recovering participants' performance on their
                                data when using federated learning with
                                robustness and privacy techniques.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/2002.04758"
                                     class="collection-item">[Paper]</a>,
                                <a target="_blank"
                                   href="https://github.com/ebagdasa/federated_adaptation"
                                   class="collection-item">[Code]</a>,
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header"><i
                                class="material-icons">done</i>Ancile: Enhancing
                            Privacy for Ubiquitous Computing with Use-Based
                            Privacy
                        </div>
                        <div class="collapsible-body">
                            <p>A novel platform that enables control over
                                application's data usage with language level
                                policies
                                and implementing use-based privacy.</p>
                            <span><a target="_blank"
                                     href="assets/files/ancile.pdf"
                                     class="collection-item">[WPES'19]</a>,
                                <a target="_blank"
                                   href="https://github.com/ancile-project/ancile"
                                   class="collection-item">[Code]</a>,
                                <a target="_blank"
                                   href="https://docs.google.com/presentation/d/1a2zkEvXLzWJ-CWu2AGl_Bc2pUwKEnnoEKyc17MZbmFQ/edit?usp=sharing"
                                   class="collection-item">[Slides]</a>.
                            </span>
                        </div>
                    </li>
                    <li>
                        <div class="collapsible-header"><i
                                class="material-icons">face</i>Differential
                            Privacy Has Disparate Impact on Model Accuracy
                        </div>
                        <div class="collapsible-body">
                            <p>This project discusses a new trade off between
                                privacy and fairness. We observe
                                that training a Machine Learning model with
                                Differential Privacy reduces accuracy on
                                underrepresented groups.</p>
                            <span><a target="_blank"
                                     href="https://arxiv.org/abs/1905.12101"
                                     class="collection-item">[NeurIPS, 2019]</a>,
                            <a target="_blank"
                               href="https://github.com/ebagdasa/differential-privacy-vs-fairness"
                               class="collection-item">
                                [Code]</a>
                            </span>
                        </div>
                    </li>

                    <li>
                        <div class="collapsible-header"><i
                                class="material-icons">extension</i>Openrec: A
                            modular framework for extensible and adaptable
                            recommendation algorithms
                        </div>
                        <div class="collapsible-body">
                            <p>An open and modular Python framework that
                                supports extensible and adaptable research in
                                recommender systems.
                            </p>
                            <a target="_blank"
                               href="https://dl.acm.org/citation.cfm?id=3159681"
                               class="collection-item">[WSDM, 2018]</a>,
                            <a target="_blank"
                               href="https://github.com/ylongqi/openrec"
                               class="collection-item">[Code]</a>
                        </div>
                    </li>
                </ul>
            </div>
        </div>

        <div class="col push-s1 s10 m5 l5 push-l1 push-m1">
            <div class="section">
                <h5>Recent news</h5>
                <ul class="collection">
                    <li class="collection-item">
                        <b>Feb 2021</b>, <a target="_blank"
                                            href="https://arxiv.org/abs/2005.03823">paper</a>
                        on backdoors was accepted to USENIX Security'21.
                    </li>
                    <li class="collection-item">
                        <b>Jan 2021</b>, presented our work at Microsoft
                        Research.
                    </li>
                    <li class="collection-item">
                        <b>Nov 2020</b>, open sourced our new
                        <a target="_blank"
                           href="https://github.com/ebagdasa/backdoors101">framework</a>
                        for research on backdoors in deep learning.
                    </li>
                    <li class="collection-item">
                        <b>Jul 2020</b>, presented our <a target="_blank"
                                                          href="https://arxiv.org/abs/2002.04758">work</a>
                        on local adaption for Federated Learning at Google.
                    </li>
                    <li class="collection-item">
                        <b>Jun 2020</b>, Ancile project was discussed in <a
                            target="_blank"
                            href="https://news.cornell.edu/stories/2020/06/platform-empowers-users-control-their-personal-data">Cornell
                        Chronicle</a>.
                    </li>
                    <li class="collection-item">
                        <b>Summer 2020</b>, interned at Google Research with <a
                            target="_blank"
                            href="http://www.winlab.rutgers.edu/~gruteser/">Marco
                        Gruteser</a>
                        and <a target="_blank"
                               href="https://research.google/people/105175/">Kaylee
                        Bonawitz</a>, focused on Federated Learning and
                        Analytics.
                    </li>
                    <li class="collection-item">
                        <b>Jan 2020</b>, our <a target="_blank"
                                                href="https://arxiv.org/abs/1807.00459">attack</a>
                        on federated learning was accepted to AISTATS'20!
                    </li>
                    <li class="collection-item">
                        <b>Nov 2019</b>, passed A exam (pre-candidacy):
                        "Evaluating privacy preserving techniques in machine
                        learning."
                    </li>
                    <li class="collection-item">
                        <b>Sep 2019</b>, our <a target="_blank"
                                                href="https://arxiv.org/abs/1905.12101">paper</a>
                        about
                        differential privacy impact on model fairness was
                        accepted to NeurIPS'19.
                    </li>
                    <li class="collection-item">
                        <b>Aug 2019</b>, our work on the use-based privacy
                        system <a target="_blank"
                                  href="assets/files/ancile.pdf">Ancile</a> was
                        accepted to CCS WPES'19.
                    </li>
                    <li class="collection-item">
                        <b>Aug 2019</b>, presented at <a target="_blank"
                                                         href="http://privaci.info/ci_symposium.html">Contextual
                        Integrity Symposium</a>
                        on contextual recommendation sharing.
                    </li>
                    <li class="collection-item">
                        <b>June 2019</b>, <a target="_blank"
                                             href="https://www.dli.tech.cornell.edu/doctoralfellows">Digital
                        Life Initiative</a> fellow 2019-2020.
                    </li>
                    <li class="collection-item">
                        <b>Summer 2018</b>, interned at Amazon Research with
                        Pawel Matykiewicz and Amber Roy Chowdhury.
                    </li>
                    <li class="collection-item">
                        <b>Sep 2017</b>,
                        <a target="_blank"
                           href="https://medium.com/@nycmedialab/data-for-good-bloomberg-supports-data-scientists-work-with-nonprofits-and-municipalities-to-solve-6d9ce6360ea8">
                            Bloomberg Data for Good</a> fellow 2017.
                    </li>
                </ul>

            </div>
        </div>
        <!--        <div class="col push-s1 s10 m6 l6">-->
        <!--            <div class="divider"></div>-->

        <!--        </div>-->
    </div>
    <!--    <div class="footer-copyright right-align"><i>last update April 2021.</i></div>-->
</div>


<!-- Compiled and minified JavaScript -->
<script type="text/javascript" src="assets/js/jquery-3.4.1.min.js"></script>
<script src="assets/js/materialize.min.js"></script>
<script type="text/javascript" src="assets/js/script.js"></script>
</body>

</html>
