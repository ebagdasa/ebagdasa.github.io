<!DOCTYPE html>
<html>
<head>
    <!--Import Google Icon Font-->
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!--Import materialize.css-->
    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@materializecss/materialize@2.1.0/dist/css/materialize.min.css">
    <!--Let browser know website is optimized for mobile-->
    <title>COMPSCI 692PA</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
</head>
<body>
<!--JavaScript at end of body for optimized loading-->
<!-- Compiled and minified JavaScript -->
<script src="https://cdn.jsdelivr.net/npm/@materializecss/materialize@2.1.0/dist/js/materialize.min.js"></script>

<div class="container">
    <h2>COMPSCI 692PA. Advanced Topics on Security and Privacy for Generative Models</h2>

    <p>New capabilities of language and diffusion models enable applications that interact with users across different modalities,
        perform independent actions, and leverage external tools. In the seminar, we will study how these capabilities
        create new privacy and security challenges by analyzing recent papers in ML and S&P communities and connecting
        discovered problems to fundamental issues from previous decades. As part of the course there will be an
        opportunity to conduct a research project that goes deeper into these problems.</p>

    <b>Prerequisites: This seminar is tailored for PhD students, but send me email if you have questions: eugene@cs.umass.edu</b>
    <p>
        <b>Instructor</b>: Eugene Bagdasaryan
    </p>
    <p>
    <b>Location</b>: Tuesdays: 8:30AM-9:30AM
    </p>
    <p>
        <b>Room</b>: CS 140
    </p>
    <p>
        <a target="_blank" href="https://umass-amherst.zoom.us/j/95228637491"><b>Zoom link</b></a> (UMass only)
    </p>
    <h3>Topics</h3>
    <div>
        <ul class="collection">
<!--            <li class="collection-item">Week X.-->
<!--                <p> <a target="_blank" href="">(link)</a> <br>-->
<!--                   extra.  <a target="_blank" href="">(link)</a> <br> </p>-->
<!--            </li>-->
            <li class="collection-item">Week 1. Intro: Why Security and Privacy for Generative Models?</li>
            <li class="collection-item">Week 2. Prompt stealing
                <p>"Effective Prompt Extraction from Language Models", Zhang et al, COLM'24 <a target="_blank" href="https://arxiv.org/abs/2307.06865">(link)</a> (Dzung) <br>
                extra: "Extracting Prompts by Inverting LLM Outputs", Zhang et al, Preprint'24 <a target="_blank" href="https://arxiv.org/pdf/2405.15012">(link)</a> </p>
            </li>
            <li class="collection-item">Week 3. Privacy. Membership Inference and Reconstruction Attacks
                <p>"Extracting Training Data from Diffusion Models", Carlini et al, USENIX'23 <a target="_blank" href="https://arxiv.org/abs/2301.13188">(link)</a> <br>
                   extra: "Membership Inference Attacks From First Principles", Carlini et al, S&P'22 <a target="_blank" href="https://arxiv.org/abs/2112.03570">(link)</a>
                </p>
            </li>

            <li class="collection-item">Week 4. Privacy. Model Stealing
                <p>"Imitation attacks and defenses for black-box machine translation systems", Wallace et al, 	EMNLP'20 <a target="_blank" href="https://arxiv.org/abs/2004.15015">(link)</a> <br>
                    extra: "The false promise of imitating proprietary LLMs", Gudibande et al, ICLR'24 <a target="_blank" href="https://arxiv.org/abs/2305.15717">(link)</a>
                </p>
            </li>
<!--            <li class="collection-item">Week 5. Privacy. Differential Privacy and Auditing</li>-->
            <li class="collection-item">Week 5. Privacy. DP In-Context Learning
            <p>
                "Privacy-Preserving In-Context Learning for Large Language Models.". Wu et al, ICLR'24 <a target="_blank" href=https://openreview.net/forum?id=x4OPJ7lHVU">(link)</a> <br>
            </p>
            </li>
            <li class="collection-item">Week 6. Privacy. Synthetic Data Generation.
                <p>Differentially Private Synthetic Data via Foundation Model APIs. Lin et al. ICLR'24 <a target="_blank" href=https://arxiv.org/abs/2305.15560">(link)</a> <br>
            </li>
            <li class="collection-item">Week 7. Privacy. Inference Privacy and Contextual Integrity</li>
            <li class="collection-item">Week 8. Mid-term project proposals.</li>
            <li class="collection-item">Week 9. Security. Jailbreaking</li>
            <li class="collection-item">Week 10. Security. Agents and Tool Use Attacks </li>
            <li class="collection-item">Week 11. Security. Visual Language Model Attacks </li>
            <li class="collection-item">Week 12. Security. Backdoor Attacks and Watermarking </li>
            <li class="collection-item">Week 13. Security. Denial of Service Attacks </li>
            <li class="collection-item">Week 14. Security. System Design Discussions </li>
            <li class="collection-item">Week 15. Presentations</li>
        </ul>
    </div>

    <h3>Project Proposals (to be picked by end of Week 4)</h3>
    <div>
        <ul class="collection">
            <li class="collection-item">TBD</li>
        </ul>
    </div>

</div>

</body>
</html>
